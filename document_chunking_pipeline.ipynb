{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion & Chunking Pipeline\n",
    "\n",
    "This notebook handles:\n",
    "1. Ingesting ~10 documents from a local directory\n",
    "2. Cleaning text (whitespace, special chars, encoding issues)\n",
    "3. Chunking with `chunk_size=600`, `overlap=100`\n",
    "4. Computing & logging statistics\n",
    "5. Printing sample chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "CHUNK_SIZE = 600      # characters per chunk\n",
    "CHUNK_OVERLAP = 100   # overlap between consecutive chunks\n",
    "DOCS_DIR = \"./documents\"  # <-- UPDATE to your documents folder\n",
    "SUPPORTED_EXTENSIONS = [\".txt\", \".md\", \".pdf\", \".docx\", \".html\", \".csv\", \".json\"]\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"chunking_pipeline.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Pipeline started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(filepath: str) -> str:\n",
    "    \"\"\"Load a single document and return its raw text.\"\"\"\n",
    "    ext = Path(filepath).suffix.lower()\n",
    "    \n",
    "    if ext in (\".txt\", \".md\", \".csv\"):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            return f.read()\n",
    "    \n",
    "    elif ext == \".json\":\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # Flatten JSON to text\n",
    "        return json.dumps(data, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    elif ext == \".html\":\n",
    "        try:\n",
    "            from bs4 import BeautifulSoup\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "            return soup.get_text(separator=\" \")\n",
    "        except ImportError:\n",
    "            logger.warning(\"bs4 not installed; reading HTML as raw text\")\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                return f.read()\n",
    "    \n",
    "    elif ext == \".pdf\":\n",
    "        try:\n",
    "            import fitz  # PyMuPDF\n",
    "            doc = fitz.open(filepath)\n",
    "            return \"\\n\".join(page.get_text() for page in doc)\n",
    "        except ImportError:\n",
    "            logger.error(\"PyMuPDF not installed. Run: pip install pymupdf\")\n",
    "            return \"\"\n",
    "    \n",
    "    elif ext == \".docx\":\n",
    "        try:\n",
    "            from docx import Document\n",
    "            doc = Document(filepath)\n",
    "            return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "        except ImportError:\n",
    "            logger.error(\"python-docx not installed. Run: pip install python-docx\")\n",
    "            return \"\"\n",
    "    \n",
    "    else:\n",
    "        logger.warning(f\"Unsupported file type: {ext} â€” skipping {filepath}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def ingest_documents(docs_dir: str) -> dict[str, str]:\n",
    "    \"\"\"Scan directory and load all supported documents.\"\"\"\n",
    "    documents = {}\n",
    "    for ext in SUPPORTED_EXTENSIONS:\n",
    "        for fpath in glob.glob(os.path.join(docs_dir, f\"*{ext}\")):\n",
    "            name = os.path.basename(fpath)\n",
    "            text = load_document(fpath)\n",
    "            if text.strip():\n",
    "                documents[name] = text\n",
    "                logger.info(f\"Loaded: {name} ({len(text):,} chars)\")\n",
    "            else:\n",
    "                logger.warning(f\"Empty or unreadable: {name}\")\n",
    "    logger.info(f\"Total documents ingested: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# --- Ingest ---\n",
    "raw_docs = ingest_documents(DOCS_DIR)\n",
    "print(f\"\\nâœ… Ingested {len(raw_docs)} documents\")\n",
    "for name, text in raw_docs.items():\n",
    "    print(f\"   â€¢ {name}: {len(text):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean a document's text.\"\"\"\n",
    "    # Remove null bytes & non-printable chars (keep newlines/tabs)\n",
    "    text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]\", \"\", text)\n",
    "    # Normalize unicode whitespace\n",
    "    text = text.replace(\"\\u00a0\", \" \").replace(\"\\u200b\", \"\")\n",
    "    # Collapse multiple blank lines into one\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    # Collapse multiple spaces (but not newlines)\n",
    "    text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n",
    "    # Strip leading/trailing whitespace per line\n",
    "    text = \"\\n\".join(line.strip() for line in text.splitlines())\n",
    "    # Final strip\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "cleaned_docs = {name: clean_text(text) for name, text in raw_docs.items()}\n",
    "\n",
    "print(\"âœ… Cleaning complete\")\n",
    "for name in cleaned_docs:\n",
    "    before = len(raw_docs[name])\n",
    "    after = len(cleaned_docs[name])\n",
    "    diff = before - after\n",
    "    print(f\"   â€¢ {name}: {before:,} â†’ {after:,} chars (removed {diff:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    overlap: int = CHUNK_OVERLAP\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    Tries to break on sentence boundaries when possible.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "    \n",
    "    while start < text_len:\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        if end < text_len:\n",
    "            # Try to find a sentence boundary (., !, ?) near the end\n",
    "            boundary = -1\n",
    "            for sep in [\". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\"]:\n",
    "                pos = text.rfind(sep, start + chunk_size // 2, end)\n",
    "                if pos > boundary:\n",
    "                    boundary = pos + 1  # include the punctuation\n",
    "            if boundary > start:\n",
    "                end = boundary\n",
    "        else:\n",
    "            end = text_len\n",
    "        \n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Advance with overlap\n",
    "        start = end - overlap if end < text_len else text_len\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks: dict[str, list[str]] = {}\n",
    "for name, text in cleaned_docs.items():\n",
    "    chunks = chunk_text(text)\n",
    "    all_chunks[name] = chunks\n",
    "    logger.info(f\"Chunked {name}: {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nâœ… Chunking complete (chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
    "for name, chunks in all_chunks.items():\n",
    "    print(f\"   â€¢ {name}: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chunks = [c for chunks in all_chunks.values() for c in chunks]\n",
    "chunk_lengths = [len(c) for c in flat_chunks]\n",
    "\n",
    "stats = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\"chunk_size\": CHUNK_SIZE, \"overlap\": CHUNK_OVERLAP},\n",
    "    \"total_documents\": len(all_chunks),\n",
    "    \"total_chunks\": len(flat_chunks),\n",
    "    \"avg_chunk_length\": round(sum(chunk_lengths) / len(chunk_lengths), 1) if chunk_lengths else 0,\n",
    "    \"min_chunk_length\": min(chunk_lengths, default=0),\n",
    "    \"max_chunk_length\": max(chunk_lengths, default=0),\n",
    "    \"chunks_per_document\": {\n",
    "        name: len(chunks) for name, chunks in all_chunks.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "logger.info(f\"Total chunks: {stats['total_chunks']}\")\n",
    "logger.info(f\"Avg chunk length: {stats['avg_chunk_length']} chars\")\n",
    "logger.info(f\"Chunks per doc: {stats['chunks_per_document']}\")\n",
    "\n",
    "# Save stats to JSON\n",
    "with open(\"chunk_stats.json\", \"w\") as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"\\nðŸ“Š Pipeline Statistics\")\n",
    "print(f\"   Total documents: {stats['total_documents']}\")\n",
    "print(f\"   Total chunks:    {stats['total_chunks']}\")\n",
    "print(f\"   Avg chunk len:   {stats['avg_chunk_length']} chars\")\n",
    "print(f\"   Min chunk len:   {stats['min_chunk_length']} chars\")\n",
    "print(f\"   Max chunk len:   {stats['max_chunk_length']} chars\")\n",
    "print(f\"\\n   Chunks per document:\")\n",
    "for name, count in stats['chunks_per_document'].items():\n",
    "    print(f\"     {name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_SAMPLES = 5\n",
    "sample_indices = random.sample(range(len(flat_chunks)), min(NUM_SAMPLES, len(flat_chunks)))\n",
    "\n",
    "print(f\"\\nðŸ“ {len(sample_indices)} Sample Chunks\\n\" + \"=\" * 60)\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    chunk = flat_chunks[idx]\n",
    "    print(f\"\\n--- Sample {i} (index={idx}, length={len(chunk)} chars) ---\")\n",
    "    print(chunk[:500] + (\"...\" if len(chunk) > 500 else \"\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "```bash\n",
    "# Push to GitHub\n",
    "git add document_chunking_pipeline.ipynb chunk_stats.json chunking_pipeline.log\n",
    "git commit -m \"feat: add document chunking pipeline notebook\"\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "### Message for Annie\n",
    "\n",
    "**Subject:** Chunking pipeline ready for review\n",
    "\n",
    "Hi Annie,\n",
    "\n",
    "Here's the GitHub link: `<REPO_URL>`\n",
    "\n",
    "Quick summary:\n",
    "- **Ingested & cleaned ~10 docs** (txt, pdf, docx, html, etc.) with whitespace normalization, non-printable char removal, and encoding fixes\n",
    "- **Chunked with size=600 / overlap=100**, using sentence-boundary-aware splitting â€” stats logged to `chunk_stats.json`\n",
    "- **No blockers** so far; only dependency note: PDFs need `pymupdf` and DOCX needs `python-docx` (`pip install pymupdf python-docx`)\n",
    "\n",
    "Let me know if you want to adjust chunk size or add metadata tagging next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
